{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Игрушечный пример на TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text 1\n",
    "The domestic cat is a small, typically furry, carnivorous mammal. They are often called house cats when kept as indoor pets or simply cats when there is no need to distinguish them from other felids and felines. There are more than seventy cat breeds recognized by various cat registries.<br>\n",
    "**Text len**: 49 words\n",
    "\n",
    "#### Text 2\n",
    "Mammals include the largest animal on the planet, the blue whale. The basic body type is a terrestrial quadruped, but some mammals are adapted for life at sea, in the air, in trees, underground or on two legs. The largest group of mammals, the placentals, have a placenta, which enables the feeding of the fetus during gestation. <br>\n",
    "**Text len**: 57 words\n",
    "\n",
    "#### Text 3\n",
    "The largest organisms found on Earth can be determined according to various aspects of an organism's size, such as: mass, volume, area, length, height, or even genome size. Some organisms group together to form a superorganism (such as ants or bees), but such are not classed as single large organisms.<br>\n",
    "**Text len**: 51 words\n",
    "\n",
    "\n",
    "Найдите TF-IDF следующих слов для этих текстов:\n",
    "* cat (+cats) - для первого текста\n",
    "* mammal (+mammals) - для первого и второго текста\n",
    "* Earth - для третьего текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11210329476205202\n",
      "0.008274798124656415\n",
      "0.021340268847798126\n",
      "0.021541417424864897\n"
     ]
    }
   ],
   "source": [
    "print((5 / 49) * log(3 / 1))\n",
    "print((1 / 49) * log(3 / 2))\n",
    "print((3 / 57) * log(3 / 2))\n",
    "print((1 / 51) * log(3 / 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обратный индекс\n",
    "\n",
    "## Задание 1.\n",
    "Напишите функцию (или набор функций), которая считает метрику Okapi BM25 для одного слова в запросе (формулы можно посмотреть в презентации)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3210731127394419\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "k1 = 2.0\n",
    "b = 0.75\n",
    "\n",
    "def score_BM25(n, qf, N, dl, avdl):\n",
    "    \"\"\"\n",
    "    Computes Okapi BM25 for a particular document and one word in a query.\n",
    "    NB: min IDF 0 (use built-in max function)\n",
    "    :param n: number of docs with a word\n",
    "    :param qf: raw word frequence in doc\n",
    "    :param N: number of docs in a collection\n",
    "    :param dl: doc length (in words)\n",
    "    :param avdl: average doc length in a collection\n",
    "    :return: float: BM25 score\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    tf = (qf * (k1 + 1)) / (qf + k1 * (1 - b + b * (dl / avdl)))\n",
    "    dlf = log((N - n + 0.5) / (n + 0.5))\n",
    "    return tf * dlf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот значения, которые получились у меня при следующих параметрах в моем коде:\n",
    "```\n",
    "print(score_BM25(n=15, qf=5, N=100, dl=120, avdl=80))\n",
    "3.30518003616293\n",
    "\n",
    "print(score_BM25(n=40, qf=1, N=100, dl=120, avdl=80))\n",
    "0.3210731127394419\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2.\n",
    "Напишите функцию, которая по списку токенизированных документов возвращает обратный индекс, то есть словарь, где для каждого слова есть список документов, в которых оно есть. В индексе можно сохранять не только названия документов, но и любые другие данные, которые вам покажется полезным сохранить (вспомните Okapi BM25 и что для ее подсчета нужно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': {'1.txt': [2, 0.6666666666666666], '3.txt': [1, 0.2]},\n",
       " 'dog': {'1.txt': [1, 0.3333333333333333],\n",
       "  '2.txt': [1, 0.3333333333333333],\n",
       "  '3.txt': [1, 0.2]},\n",
       " 'mouse': {'2.txt': [1, 0.3333333333333333], '3.txt': [2, 0.4]},\n",
       " 'home': {'2.txt': [1, 0.3333333333333333], '3.txt': [1, 0.2]}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inverted_index(docs):\n",
    "    \"\"\"\n",
    "    Compiles inverted index from document collection.\n",
    "    :param docs: dict[list][str]: dict of tokenized documents, where key is document name and value is tokenized text\n",
    "    :return: dict: inverted index\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    index_map = {}\n",
    "    for doc in docs:\n",
    "        words = docs[doc]\n",
    "        for word in words:\n",
    "            if index_map.get(word) is None:\n",
    "                index_map[word] = {}\n",
    "            if index_map[word].get(doc) is None:\n",
    "                index_map[word][doc] = [1]\n",
    "            else:\n",
    "                index_map[word][doc][0] += 1\n",
    "    for word in index_map:\n",
    "        for doc in index_map[word]:\n",
    "            index_map[word][doc].append(index_map[word][doc][0] / len(docs[doc]))\n",
    "    return index_map\n",
    "\n",
    "docs = {\n",
    "    '1.txt': 'cat dog cat'.split(),\n",
    "    '2.txt': 'dog mouse home'.split(),\n",
    "    '3.txt': 'cat dog mouse mouse home'.split(),\n",
    "}\n",
    "inverted_index(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример:\n",
    "```\n",
    "docs = {\n",
    "    '1.txt': 'cat dog cat'.split(),\n",
    "    '2.txt': 'dog mouse home'.split(),\n",
    "    '3.txt': 'cat dog mouse mouse home'.split()\n",
    "}\n",
    "\n",
    "inverted_index(docs)\n",
    "```\n",
    "Результат:\n",
    "```\n",
    "{'cat': {'1.txt': 2, '3.txt': 1},\n",
    " 'dog': {'1.txt': 1, '2.txt': 1, '3.txt': 1},\n",
    " 'home': {'2.txt': 1, '3.txt': 1},\n",
    " 'mouse': {'2.txt': 1, '3.txt': 2}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дополнительное задание\n",
    "\n",
    "Напишите функцию, которая для каждой статьи из нашего корпуса `lenta_articles.txt` выделяет ключевые слова и биграммы (полезно - `nltk.bigrams`) на основе TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "NO\n"
     ]
    }
   ],
   "source": [
    "s = str(input())\n",
    "if (s == s[::-1]):\n",
    "    print(\"YES\")\n",
    "else:\n",
    "    print(\"NO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "\n",
    "## Задание 1.\n",
    "\n",
    "Прочитайте корпус, предобработайте каждый документ и сделайте (и сохраните в формате json) обратный индекс. Обратный индекс - словарь, где для каждого слова из корпуса есть список документов, в которых оно есть. Также подумайте, какую информацию по корпусу вам нужно сохранить (вспомните, что нужно для подсчета Okapi BM25) и сохраните ее тоже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "mystem_analyzer = Mystem()\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "\n",
    "def preprocessing(text):\n",
    "    textarr = mystem_analyzer.analyze(text);\n",
    "    preprocessed_text = \"\"\n",
    "    for i in textarr:\n",
    "        if not i.get('analysis') is None and len(i.get('analysis')) > 0:\n",
    "            if not i.get('analysis')[0].get('lex') in stopwords.words('russian'):\n",
    "                preprocessed_text += i.get('analysis')[0].get('lex').lower()\n",
    "        else:\n",
    "            preprocessed_text += i.get('text').lower()\n",
    "    for sgn in punctuation + '«»—':\n",
    "        preprocessed_text = preprocessed_text.replace(sgn, '')\n",
    "    return preprocessed_text\n",
    "\n",
    "def inverted_index(docs):\n",
    "    index_map = {}\n",
    "    for doc in docs:\n",
    "        words = word_tokenize(doc[\"text\"])\n",
    "        for word in words:\n",
    "            if index_map.get(word) is None:\n",
    "                index_map[word] = {}\n",
    "            if index_map[word].get(doc['url']) is None:\n",
    "                index_map[word][doc['url']] = [1]\n",
    "            else:\n",
    "                index_map[word][doc['url']][0] += 1\n",
    "    for word in index_map:\n",
    "        i = 0\n",
    "        for doc in index_map[word]:\n",
    "            a = index_map[word][doc][0]\n",
    "            b = len(word_tokenize(docs[i]['text']))\n",
    "            index_map[word][doc].append(a / b)\n",
    "            i += 1\n",
    "    docs_map = {}\n",
    "    for doc in docs:\n",
    "        docs_map[doc['url']] = {\"size\": len(word_tokenize(doc['text'])), \"theme\": doc['theme'], \"header\": doc['header']} \n",
    "    return {'words': index_map, 'docs': docs_map}\n",
    "\n",
    "\n",
    "def docs_get(s):\n",
    "    docs = []\n",
    "    l = s.split('=' * 50 + '\\n')\n",
    "    f = 0\n",
    "    k = 0\n",
    "    for i in l:\n",
    "        i = i.split('\\n');\n",
    "        doc = {}\n",
    "        doc['theme'] = i[0]\n",
    "        doc['date'] = i[1]\n",
    "        doc['url'] = i[2]\n",
    "        doc['header'] = i[3]\n",
    "        doc['text'] = preprocessing(' '.join(i[3:]))\n",
    "        docs.append(doc)\n",
    "    return {'size': len(l), 'value': docs}\n",
    "    \n",
    "\n",
    "with open('articles.txt', 'r', encoding='utf-8') as f:\n",
    "    s = f.read()\n",
    "docs = docs_get(s)\n",
    "index = {'size': docs['size'], 'value': inverted_index(docs['value'])}\n",
    "json.dump(index, open('articles.json', 'w', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2.\n",
    "\n",
    "Напишите функцию, которая по запросу выдает топ-10 наиболее релевантных документов.\n",
    "* предобработка запроса\n",
    "* поиск слов по обратному индексу\n",
    "* подсчет метрики BM-25\n",
    "* сортировка документов\n",
    "\n",
    "(можно писать вспомогательные функции)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кто сел на обратной стороне луны?\n",
      "По запросу найдено 29 соответствий\n",
      "На обратной стороне Луны впервые сели https://lenta.ru/news/2019/01/03/china/\n",
      "Опубликовано первое фото с обратной стороны Луны https://lenta.ru/news/2019/01/03/sputnik/\n",
      "Директор российского Burger King подвезла попутчика и пропала https://lenta.ru/news/2018/12/27/blablacar/\n",
      "Названы сроки создания российской сверхтяжелой ракеты для полета на Луну https://lenta.ru/news/2019/01/04/rocket/\n",
      "WADA даст время российскому спорту https://lenta.ru/news/2019/01/04/wada/\n",
      "Рыбаки на сломанной лодке застряли в кишащей крокодилами реке https://lenta.ru/news/2018/12/28/crocoriver/\n",
      "Темная сторона Поднебесной https://lenta.ru/articles/2018/12/31/guizhou/\n",
      "Российским лыжникам напророчили санкции на чемпионате мира-2019 https://lenta.ru/news/2019/01/04/mok/\n",
      "Уходят с орбиты https://lenta.ru/articles/2019/01/02/space2019/\n",
      "Вегетарианство назвали губительным для всего человечества https://lenta.ru/news/2018/12/27/vegan/\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "from pymystem3 import Mystem\n",
    "mystem_analyzer = Mystem()\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "\n",
    "def preprocessing(text):\n",
    "    textarr = mystem_analyzer.analyze(text);\n",
    "    preprocessed_text = \"\"\n",
    "    for i in textarr:\n",
    "        if not i.get('analysis') is None and len(i.get('analysis')) > 0:\n",
    "            if not i.get('analysis')[0].get('lex') in stopwords.words('russian'):\n",
    "                preprocessed_text += i.get('analysis')[0].get('lex').lower()\n",
    "        else:\n",
    "            preprocessed_text += i.get('text').lower()\n",
    "    for sgn in punctuation + '«»—':\n",
    "        preprocessed_text = preprocessed_text.replace(sgn, '')\n",
    "    return preprocessed_text\n",
    "\n",
    "def score_BM25(n, qf, N, dl, avdl):\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    tf = (qf * (k1 + 1)) / (qf + k1 * (1 - b + b * (dl / avdl)))\n",
    "    dlf = log((N - n + 0.5) / (n + 0.5))\n",
    "    return tf * dlf\n",
    "\n",
    "def average(docs):\n",
    "    k = 0\n",
    "    l = 0\n",
    "    for doc in docs:\n",
    "        l += docs[doc]['size']\n",
    "        k += 1\n",
    "    return l / k\n",
    "\n",
    "def score(index, word, doc):\n",
    "    if not index['value']['words'].get(word) is None:\n",
    "        n = len(index['value']['words'][word])\n",
    "        if not index['value']['words'][word].get(doc) is None:\n",
    "            qf = index['value']['words'][word][doc][1]\n",
    "        else:\n",
    "            qf = 0\n",
    "    else:\n",
    "        n = 0\n",
    "        qf = 0\n",
    "    N = index['size']\n",
    "    dl = index['value']['docs'][doc]['size']\n",
    "    avdl = average(index['value']['docs'])\n",
    "    return score_BM25(n, qf, N, dl, avdl)\n",
    "\n",
    "index = json.load(open('articles.json'))\n",
    "query = str(input())\n",
    "query = word_tokenize(preprocessing(query))\n",
    "rating = []\n",
    "for doc in index['value']['docs']:\n",
    "    current_rating = 0\n",
    "    current_words_quanity = 0\n",
    "    for word in query:\n",
    "        current_score = score(index, word, doc)\n",
    "        if current_score > 0:\n",
    "            current_words_quanity += 1\n",
    "        current_rating += current_score\n",
    "    rating.append([current_rating * current_words_quanity, doc])\n",
    "\n",
    "rating.sort()\n",
    "i = len(rating) -1\n",
    "while i >= 0 and rating[i][0] > 0:\n",
    "    i -= 1\n",
    "ans = (len(rating) - i - 1)\n",
    "if ans % 10 == 1 and ans // 10 % 10 != 1:\n",
    "    ans_str = \"соответствие\"\n",
    "elif ans % 10 in [2, 3, 4] and ans // 10 % 10 != 1:\n",
    "    ans_str = \"соответствия\"\n",
    "else:\n",
    "    ans_str = \"соответствий\"\n",
    "print('По запросу найдено', len(rating) - i - 1, ans_str)\n",
    "for i in range(len(rating) - 1, max(len(rating) - 11, i), -1):\n",
    "    print(index['value']['docs'][rating[i][1]]['header'], rating[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
